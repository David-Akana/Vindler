{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST TASK: CLASSIFY FACES FROM NONE FACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset path\n",
    "dataset = 'faceornot/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check image names\n",
    "for picture in listdir(dataset) :\n",
    "     if picture != 'Thumbs.db':\n",
    "            print(picture)#print(picture[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates features and targets\n",
    "def load_images(directory):\n",
    "    train_pictures = list()\n",
    "    targets = list()\n",
    "    gray_scaled = list()\n",
    "    images = pd.DataFrame()\n",
    "    print('Converting to numpy array ...')\n",
    "    for picture in listdir(directory) :\n",
    "         if picture != 'Thumbs.db':\n",
    "                 # load the picture from directory\n",
    "                photo = load_img(directory + picture, target_size = (224,224))\n",
    "                \n",
    "                #convert image to numpy array\n",
    "                photo = img_to_array(photo, dtype='uint8')  \n",
    "                \n",
    "                #photo = photo.reshape(1, 224, 224, 3)\n",
    "                \n",
    "                # append to list\n",
    "                #train_pictures.append(photo)\n",
    "                \n",
    "                # convert image to grayscale\n",
    "                gray_scale = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)\n",
    "                gray_scaled.append(gray_scale)\n",
    "                \n",
    "                # label targets\n",
    "                if picture[0] == 'f':\n",
    "                    targets.append(1)\n",
    "                else:\n",
    "                    targets.append(0)\n",
    "    print(' ')\n",
    "    #print('Converting to Pandas dataframe ...') \n",
    "\n",
    "    #images['image_arrays'] = gray_scaled\n",
    "    #images['targets'] = targets\n",
    "    \n",
    "    X_TRAIN = asarray(gray_scaled, dtype = 'uint8')\n",
    "    Y_TRAIN = asarray(targets, dtype = 'uint8')\n",
    "    print(' ')\n",
    "    print('Done!')\n",
    "    return X_TRAIN, Y_TRAIN #images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "X,Y = load_images(dataset) #images = load_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[47])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape((80, 224*224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_x = StandardScaler()\n",
    "#x_train = sc_x.fit_transform(x_train)\n",
    "#x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the dimension of the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()  #kernel ='rbf' # this takes the indpendent variables that interpret the dataset the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_check = pca.fit(x_train)  #x_train_check = pca.fit_transform(x_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance # this was used to know how many components the PCA would explain \"the most the variance\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(explained_variance) # find the cummulative summation of that array\n",
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = np.argmax(cumsum >= 0.96) + 1 # return the indices in the cumsum with a value greater than 0.96 ... this value is 40( this is 6\n",
    "# because of pythons indexing, then add plus 1: 41. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = n_comp)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically the eigen faces are the vectors that best represent the large image matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape((n_comp, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us convert back to the original images from the compressed format.\n",
    "X_recover = pca.inverse_transform(x_train)\n",
    "X_recover= X_recover.reshape((64,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying using Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "Fclassifier = SVC()\n",
    "#Fclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using grid search to obtain optimal hyperparameters \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C' : [1,100,1000], 'kernel':['linear']},\n",
    "              {'C' : [1,100,1000], 'kernel':['rbf'], 'gamma':[0.5,0.1, 0.01,0.001,0.0001]}]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = Fclassifier, \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs =-1)\n",
    "grid_search = grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_\n",
    "best_accuracy * 100 # Multiplied by 100 to get in percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = [{'C' : [1,100,1000], 'kernel':['linear']},\n",
    "              {'C' : [1,100,1000], 'kernel':['rbf'], 'gamma':[0.5,0.1, 0.01,0.001,0.0001]}]\n",
    "\n",
    "\n",
    "random_search =  RandomizedSearchCV(Fclassifier,parameters,n_iter =100, random_state = 0, verbose = 1)\n",
    "\n",
    "random_search = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = random_search.best_score_\n",
    "best_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters for randomsearch\n",
    "best_parametersR = random_search.best_params_\n",
    "best_parametersR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters for grid search\n",
    "best_parameters = grid_search.best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fclassifier = grid_search.best_estimator_\n",
    "Fclassifier.fit(x_train, y_train)#.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Fclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cn = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred, y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the x_test look like?\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recover = pca.inverse_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recover = test_recover.reshape((16,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_recover[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND TASK: IDENTIFY WHO IS IN THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset path\n",
    "dataset = 'whoisthis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = pd.read_csv('name_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names(name_file):\n",
    "    name_tag= dict()\n",
    "    for i in range(len(name_file)):\n",
    "        name, tags = name_file[\"image_name\"][i], name_file['tags'][i]\n",
    "        name_tag[name] = tags\n",
    "    return(name_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see = names(name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_passcode(list_):\n",
    "    \n",
    "    maps = dict() \n",
    "    for i  in list_:\n",
    "        maps[i] = secrets.token_hex(24)\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_targets(directory, name_file):\n",
    "    #train_pictures = list()\n",
    "    targets = list()\n",
    "    gray_scaled = list()\n",
    "    name_targ = names(name_file)\n",
    "    images = pd.DataFrame()\n",
    "    passcode_targets = list()\n",
    "    print('Converting to numpy array ...')\n",
    "    for picture in listdir(directory) :\n",
    "         if picture != 'Thumbs.db':\n",
    "                 # load the picture from directory\n",
    "                photo = load_img(directory + picture, target_size = (224,224))\n",
    "                \n",
    "                #convert image to numpy array\n",
    "                photo = img_to_array(photo, dtype='uint8')  \n",
    "                \n",
    "                #photo = photo.reshape(1, 224, 224, 3)\n",
    "                \n",
    "                # append to list\n",
    "                #train_pictures.append(photo)\n",
    "                \n",
    "                # convert image to grayscale\n",
    "                gray_scale = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)\n",
    "                gray_scaled.append(gray_scale)\n",
    "                \n",
    "                # label targets\n",
    "                targets.append(name_targ[picture.split('.')[0]])\n",
    "\n",
    "    print(' ')\n",
    "    maps = name_passcode(targets)\n",
    "    \n",
    "    for i in targets:\n",
    "        passcode_targets.append(maps[i])\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    X_TRAIN = asarray(gray_scaled, dtype = 'uint8')\n",
    "    Y_TRAIN = asarray(passcode_targets)#, dtype = 'uint8')\n",
    "    print(' ')\n",
    "    print('Done!')\n",
    "    return X_TRAIN, Y_TRAIN, maps #images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y, mapper = load_images_targets(dataset, name_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESHAPE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for keys in mapper:\n",
    "    print(keys, '==>', mapper[keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an inverse mapper\n",
    "inv_mapper = dict()\n",
    "for keys in mapper:\n",
    "    inv_mapper[mapper[keys]] = keys\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = np.array(X).reshape((150, 224*224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train,y_test = train_test_split(X_r,Y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDUCING THE DIMENSION OF THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()  #kernel ='rbf' # this takes the indpendent variables that interpret the dataset the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_check = pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance # this was used to know how many components the PCA would explain \"the most the variance\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum1 = np.cumsum(explained_variance) # find the cummulative summation of that array\n",
    "cumsum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = np.argmax(cumsum1 >= 0.99) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1= PCA(n_components = n_comp)\n",
    "x_train = pca1.fit_transform(x_train)\n",
    "x_test = pca1.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca1.components_.reshape((n_comp, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eigenfaces[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us convert back to the original images from the compressed format.\n",
    "X_recover = pca1.inverse_transform(x_train)\n",
    "X_recover= X_recover.reshape((120,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recover[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSIFYING USING SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "Wclassifier = SVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters =   [{'C' : [1,100,1000], 'kernel':['linear']},\n",
    "              {'C' : [1,100,1000], 'kernel':['rbf'], 'gamma':[0.5,0.1, 0.01,0.001,0.0001]}]\n",
    "\n",
    "             \n",
    "\n",
    "grid_search = GridSearchCV(estimator = Wclassifier, \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 6,\n",
    "                           n_jobs =-1)\n",
    "grid_search = grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_\n",
    "best_accuracy * 100 # Multiplied by 100 to get in percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = [{'C' : [1,100,1000], 'kernel':['linear']},\n",
    "              {'C' : [1,100,1000], 'kernel':['rbf'], 'gamma':[0.5,0.1, 0.01,0.001,0.0001]}]\n",
    "\n",
    "\n",
    "random_search =  RandomizedSearchCV(Wclassifier,parameters,n_iter =100, random_state = 0, verbose = 1)\n",
    "\n",
    "random_search = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = random_search.best_score_\n",
    "best_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters for randomsearch\n",
    "best_parametersR = random_search.best_params_\n",
    "best_parametersR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters for gridsearch\n",
    "best_parametersG = grid_search.best_params_\n",
    "best_parametersG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wclassifier = grid_search.best_estimator_\n",
    "Wclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Wclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cn = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred, y_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SGDClassifier as a linear Support Vector Machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A stochastic gradient descent with hyper-parameters such as loss and penality set to 'hinge' and 'l2' i.e ridge regularization\n",
    "# respectively is equivalent to a linear support vector machines classifier.\n",
    "\n",
    "# This is important because of the online learning capabilty (i.e .partial_fit() ) the SGDClassifier provides\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "Wclassifier2 = SGDClassifier(loss=\"hinge\", penalty=\"l2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wclassifier2.partial_fit(x_train, y_train, classes=np.unique(y_train))\n",
    "\n",
    "# This link explains the use of classes above: https://stackoverflow.com/questions/42147302/sklearn-sgd-partial-fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = Wclassifier2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn2 = confusion_matrix(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred2, y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using .fit() both models perform exactly the same, but when using .partial_fit() it performs lower than the normal svm. \n",
    "# This is a problem more data can solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the predictions for the normal SVM Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert back to the names\n",
    "y_test =[inv_mapper[i] for i in y_test]\n",
    "y_pred =[inv_mapper[i] for i in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(y_pred, y_test)\n",
    "print('Predicted | Actual')\n",
    "print('-------------------')\n",
    "for l1,l2 in zipped:\n",
    "    print(f'- {l1} | {l2} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the predictions for the SGD Classifier(linear SVM Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 =[inv_mapper[i] for i in y_pred2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(y_pred2, y_test)\n",
    "print('Predicted | Actual')\n",
    "print('-------------------')\n",
    "for l1,l2 in zipped:\n",
    "    print(f'- {l1} | {l2} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING LBPHFACERECOGINIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LBPH is used to see if the model will perform a lot better than SVC on a the given dataset . (maybe like 30+ increase in accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_targets(directory, name_file):\n",
    "    #train_pictures = list()\n",
    "    targets = list()\n",
    "    gray_scaled = list()\n",
    "    name_targ = names(name_file)\n",
    "    print('Converting to numpy array ...')\n",
    "    for picture in listdir(directory) :\n",
    "         if picture != 'Thumbs.db':\n",
    "                 # load the picture from directory\n",
    "                photo = load_img(directory + picture, target_size = (224,224))\n",
    "                \n",
    "                #convert image to numpy array\n",
    "                photo = img_to_array(photo, dtype='uint8')  \n",
    "                \n",
    "                #photo = photo.reshape(1, 224, 224, 3)\n",
    "                \n",
    "                # append to list\n",
    "                #train_pictures.append(photo)\n",
    "                \n",
    "                # convert image to grayscale\n",
    "                gray_scale = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)\n",
    "                gray_scaled.append(gray_scale)\n",
    "                \n",
    "                # label targets\n",
    "                targets.append(name_targ[picture.split('.')[0]])\n",
    "\n",
    "\n",
    "            \n",
    "    X_TRAIN = asarray(gray_scaled, dtype = 'uint8')\n",
    "    Y_TRAIN = asarray(targets)#, dtype = 'uint8')\n",
    "    print(' ')\n",
    "    print('Done!')\n",
    "    return X_TRAIN, Y_TRAIN #images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_images_targets(dataset, name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The  LBPHFACERECOGINIZER does not take categorical targets so I converted to integers.\n",
    "hold = Y.tolist() # convert array to list\n",
    "uniq = set(hold) # remove duplicates\n",
    "names = list(uniq) # convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = {names[i]:i for i in range(len(names))} # creat dictionary mapping names to numbers\n",
    "inv_mapped = {i:names[i] for i in range(len(names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Y\n",
    "Y_i = [mapped[i] for i in hold]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the targets and the feature are still mapped correctly Lets check the first, last and any random image\n",
    "\n",
    "# The first\n",
    "plt.imshow(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Second\n",
    "plt.imshow(X[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_i[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Last\n",
    "plt.imshow(X[119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_i[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y_i, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_i[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.reshape((120, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.train(x_train, np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = list()\n",
    "for i in x_test:\n",
    "    y_p,_ = clf.predict(i)\n",
    "    y_pred.append(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred, y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [inv_mapped[i] for i in y_pred]\n",
    "y_test = [inv_mapped[i] for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zipped = zip(y_pred, y_test)\n",
    "print('Predicted | Actual')\n",
    "print('-------------------')\n",
    "for l1,l2 in zipped:\n",
    "    print(f'- {l1} | {l2} ')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It just increased by plus 7, which is not impressive. Best solution is get more data. Possibly +150 images per face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### REFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/hamishdickson/preprocessing-images-with-dimensionality-reduction\n",
    "\n",
    "#  https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html\n",
    "\n",
    "#  https://scikit-learn.org/stable/modules/sgd.html#sgd\n",
    "\n",
    "# https://shankarmsy.github.io/posts/pca-sklearn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
